{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Gen Tutorial\n",
    "Note book written by John Adeojo\n",
    "Founder, and Chief Data Scientist at [Data-centric Solutions](https://www.data-centric-solutions.com/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "import yaml\n",
    "import openai \n",
    "import os\n",
    "\n",
    "script_dir = \"C:/Users/johna/OneDrive/Documents/api_keys/\"\n",
    "index_path = \"G:/My Drive/Data-Centric Solutions/07. Blog Posts/AutoGen/autogen_tutorial/indexes/\"\n",
    "configurations_path = \"G:/My Drive/Data-Centric Solutions/07. Blog Posts/AutoGen/autogen_tutorial/\"\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"configurations.json\",\n",
    "    file_location=configurations_path,\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo-16k\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "def get_apikey(script_dir=script_dir):\n",
    "    \"\"\"\n",
    "    Reads API key from a configuration file.\n",
    "\n",
    "    This function opens a configuration file named \"apikeys.yml\", reads the API key for OpenAI\n",
    "\n",
    "    Returns:\n",
    "    api_key (str): The OpenAI API key.\n",
    "    \"\"\"\n",
    "    # Update the script_dir path to point to the correct location in your Google Drive\n",
    "    script_dir = script_dir\n",
    "    file_path = os.path.join(script_dir, \"apikeys.yml\")\n",
    "\n",
    "    with open(file_path, 'r') as yamlfile:\n",
    "        loaded_yamlfile = yaml.safe_load(yamlfile)\n",
    "        API_KEY = loaded_yamlfile['openai']['api_key']\n",
    "\n",
    "    return API_KEY\n",
    "\n",
    "# Call the function to get the API key\n",
    "openai.api_key = get_apikey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 1: Does a query based search for Wikipages\n",
    "from typing import Any, List\n",
    "import wikipedia\n",
    "from llama_index import download_loader, VectorStoreIndex, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.text_splitter import get_default_text_splitter\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "from llama_index.program import OpenAIPydanticProgram\n",
    "from utils import get_apikey\n",
    "from typing import Callable, Dict, Optional, Union, List, Tuple, Any\n",
    "from llama_hub.wikipedia.base import WikipediaReader\n",
    "from llama_index import StorageContext\n",
    "from llama_index import load_index_from_storage\n",
    "import json\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "class WikiPediaPageName(BaseModel):\n",
    "    \"\"\"Data model for a Wikipedia Page.\"\"\"\n",
    "    page: str\n",
    "\n",
    "class WikiPageList(BaseModel):\n",
    "    \"Data model for WikiPageList\"\n",
    "    pages: List[WikiPediaPageName]\n",
    "\n",
    "def wikipage_list(wikipediapages):\n",
    "        openai.api_key = get_apikey()\n",
    "\n",
    "        prompt_template_str = \"\"\"\n",
    "        Given the input {query}, \n",
    "        return a list of Wikipedia page names.\n",
    "        \"\"\"\n",
    "        program = OpenAIPydanticProgram.from_defaults(\n",
    "            output_cls=WikiPageList,\n",
    "            prompt_template_str=prompt_template_str,\n",
    "            verbose=True,\n",
    "            llm=OpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "            \n",
    "        )\n",
    "        return program(query=wikipediapages)\n",
    "\n",
    "def search_wikipedia(\n",
    "        query: str, lang: str = \"en\", results_limit: int = 5, **load_kwargs: Any\n",
    "    ) -> List[List]:\n",
    "    wikipedia.set_lang(lang)\n",
    "    return wikipedia.search(query, results=results_limit)\n",
    "\n",
    "def create_wikidocs(wikipage_requests):\n",
    "    print(f\"Preparing to Download:{wikipage_requests}\")\n",
    "    WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "    loader = WikipediaReader()\n",
    "    documents = loader.load_data(pages=wikipage_requests)\n",
    "    return documents\n",
    "\n",
    "def save_index(index) -> None:\n",
    "    index.storage_context.persist(index_path)\n",
    "\n",
    "def index_wikipedia_pages(wikipediapages: list):\n",
    "    global index\n",
    "    wikipage_requests = wikipage_list(wikipediapages)\n",
    "    documents = create_wikidocs(wikipage_requests)\n",
    "    text_splits = get_default_text_splitter(chunk_size=150, chunk_overlap=45)\n",
    "    parser = SimpleNodeParser.from_defaults(text_splitter=text_splits)\n",
    "    service_context = ServiceContext.from_defaults(node_parser=parser)\n",
    "    index =  VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    save_index(index)\n",
    "    return print (f\"{wikipediapages} have been indexed.\")\n",
    "\n",
    "def load_index(filepath: str):\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "    # load index\n",
    "    return load_index_from_storage(storage_context)\n",
    "\n",
    "def read_json_file(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def query_vector_db(\n",
    "    search_string: str,\n",
    "    file_path = index_path,\n",
    "     n_results: int = 10\n",
    ") -> None: \n",
    "    index = load_index(filepath=index_path)\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"compact\", verbose=True, similarity_top_k=n_results\n",
    "    )\n",
    "    nodes = query_engine.query(search_string).source_nodes\n",
    "    retrieved_context = {\n",
    "        # \"ids\": [],\n",
    "        \"text\": []\n",
    "    }\n",
    "    for node in nodes:\n",
    "        # retrieved_context[\"ids\"].append(node.node.id_)\n",
    "        retrieved_context[\"text\"].append(node.node.text)\n",
    "    \n",
    "    file_path = index_path + \"retrieved_context.json\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(retrieved_context, f)\n",
    "    \n",
    "    return retrieved_context\n",
    "\n",
    "def generate_response(task:str):\n",
    "    openai.api_key = get_apikey()\n",
    "    context = read_json_file(index_path + \"retrieved_context.json\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant, you think step by step to help you respond\"},\n",
    "            {\"role\": \"user\", \"content\": task},\n",
    "            {\"role\": \"assistant\", \"content\": f\"Use the information `{context}` to help you respond to the `{task}`\"}\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"search_wikipedia\",\n",
    "            \"description\": \"Use this to search for relevant Wikipedia pages\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A query to search and identify relevant Wikipedia pages\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"name\": \"index_wikipedia_pages\",\n",
    "            \"description\": \"Use this to index wikipedia pages\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"wikipediapages\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The output of the search_wikipedia function\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"wikipediapages\"],\n",
    "            },\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"name\": \"query_vector_db\",\n",
    "            \"description\": \"Useful for retrieving relevant context from the index\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_string\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A query for which relevant context is required from the index\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_string\"],\n",
    "            },\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"name\": \"generate_response\",\n",
    "            \"description\": \"Useful to generate a response based on additional context provided by wikipedia search\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"task\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"task\": \"Your understanding of the task\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"task\"],\n",
    "            },\n",
    "        },   \n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "    \"request_timeout\": 600,\n",
    "    # \"seed\":43\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Which bank was the first to default in the 2023 US banking crisis?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mwikisearch_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: search_wikipedia *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"2023 US banking crisis\"\n",
      "}\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_wikipedia...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"search_wikipedia\" *****\u001b[0m\n",
      "['2023 United States banking crisis', 'List of banking crises', 'Venezuelan banking crisis of 1994', 'Ghana banking crisis', 'Bank run']\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-04 13:01:08] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 84650749f7dbef5fe8f84f18cdd25ee7 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 84650749f7dbef5fe8f84f18cdd25ee7 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 84650749f7dbef5fe8f84f18cdd25ee7 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:01:06 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '308', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179631', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '122ms', 'x-request-id': '84650749f7dbef5fe8f84f18cdd25ee7', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0b2daba33dca-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:01:18] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 33edda414d2c3cce66a8d93f0e8b4b9a in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 33edda414d2c3cce66a8d93f0e8b4b9a in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 33edda414d2c3cce66a8d93f0e8b4b9a in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:01:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '310', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179631', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '122ms', 'x-request-id': '33edda414d2c3cce66a8d93f0e8b4b9a', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0b6fedb13dca-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:01:29] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 0a8c7ac76515f455b3feea89a87cbed7 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 0a8c7ac76515f455b3feea89a87cbed7 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 0a8c7ac76515f455b3feea89a87cbed7 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:01:27 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '381', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179631', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '122ms', 'x-request-id': '0a8c7ac76515f455b3feea89a87cbed7', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0bb24fb23dca-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:01:40] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 5fcc662d4c559b95736e0978a7792166 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 5fcc662d4c559b95736e0978a7792166 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 5fcc662d4c559b95736e0978a7792166 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:01:38 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '508', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179631', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '122ms', 'x-request-id': '5fcc662d4c559b95736e0978a7792166', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0bf458903dca-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:01:50] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 813ed7273d7d738481080637497f7965 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 813ed7273d7d738481080637497f7965 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 813ed7273d7d738481080637497f7965 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:01:48 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '317', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179631', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '122ms', 'x-request-id': '813ed7273d7d738481080637497f7965', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0c371f1e3dca-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:02:01] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 186ba4603dce22f6b563ac84246a30f8 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 186ba4603dce22f6b563ac84246a30f8 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 186ba4603dce22f6b563ac84246a30f8 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:01:59 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '417', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179631', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '122ms', 'x-request-id': '186ba4603dce22f6b563ac84246a30f8', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0c7958df3dca-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:02:11] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 73c0d212f5f84a9ed71d8e02b05c38ae in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 73c0d212f5f84a9ed71d8e02b05c38ae in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 73c0d212f5f84a9ed71d8e02b05c38ae in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:02:09 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '237', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179631', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '122ms', 'x-request-id': '73c0d212f5f84a9ed71d8e02b05c38ae', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0cbbac773dca-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:02:22] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID c80f76e02d28b8900c5286d332f4ffda in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID c80f76e02d28b8900c5286d332f4ffda in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID c80f76e02d28b8900c5286d332f4ffda in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:02:20 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '424', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179631', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '122ms', 'x-request-id': 'c80f76e02d28b8900c5286d332f4ffda', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0cfd59f63dca-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:02:33] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 63a1a4936954c4be72ff841019e8af0e in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 63a1a4936954c4be72ff841019e8af0e in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 63a1a4936954c4be72ff841019e8af0e in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:02:31 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '184', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179631', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '122ms', 'x-request-id': '63a1a4936954c4be72ff841019e8af0e', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0d42598c4084-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "\u001b[33mwikisearch_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: index_wikipedia_pages *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"wikipediapages\": \"2023 United States banking crisis\"\n",
      "}\n",
      "\u001b[32m**********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION index_wikipedia_pages...\u001b[0m\n",
      "Function call: WikiPageList with args: {\n",
      "  \"pages\": [\n",
      "    {\n",
      "      \"page\": \"2023 United States banking crisis\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Preparing to Download:pages=[WikiPediaPageName(page='2023 United States banking crisis')]\n",
      "2023 United States banking crisis have been indexed.\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"index_wikipedia_pages\" *****\u001b[0m\n",
      "None\n",
      "\u001b[32m******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mwikisearch_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: search_wikipedia *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"2023 United States banking crisis\"\n",
      "}\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_wikipedia...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"search_wikipedia\" *****\u001b[0m\n",
      "['2023 United States banking crisis', 'Banking in the United States', '2023 United States debt-ceiling crisis', 'Acquisition of Credit Suisse by UBS', 'Ghana banking crisis']\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mwikisearch_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: index_wikipedia_pages *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"wikipediapages\": \"2023 United States banking crisis\"\n",
      "}\n",
      "\u001b[32m**********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION index_wikipedia_pages...\u001b[0m\n",
      "Function call: WikiPageList with args: {\n",
      "  \"pages\": [\n",
      "    {\n",
      "      \"page\": \"2023 United States banking crisis\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Preparing to Download:pages=[WikiPediaPageName(page='2023 United States banking crisis')]\n",
      "2023 United States banking crisis have been indexed.\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"index_wikipedia_pages\" *****\u001b[0m\n",
      "None\n",
      "\u001b[32m******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mwikisearch_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: search_wikipedia *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"2023 United States banking crisis default\"\n",
      "}\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_wikipedia...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"search_wikipedia\" *****\u001b[0m\n",
      "['2023 United States banking crisis', '2023 United States debt-ceiling crisis', 'Subprime mortgage crisis', 'Ghana banking crisis', '2007–2008 financial crisis']\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-04 13:03:07] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 39cdb7e0d1c4bdf279bb98b79dd9b304 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 39cdb7e0d1c4bdf279bb98b79dd9b304 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 39cdb7e0d1c4bdf279bb98b79dd9b304 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:03:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '363', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179538', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '154ms', 'x-request-id': '39cdb7e0d1c4bdf279bb98b79dd9b304', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0e17cc874084-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:03:18] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 2f7d86d0f3ebf46b7220c4dcbb1fd416 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 2f7d86d0f3ebf46b7220c4dcbb1fd416 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 2f7d86d0f3ebf46b7220c4dcbb1fd416 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:03:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '313', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179538', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '154ms', 'x-request-id': '2f7d86d0f3ebf46b7220c4dcbb1fd416', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0e59c8024084-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:03:28] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID ae91cf83ed2fa079c51be9851a9579ed in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID ae91cf83ed2fa079c51be9851a9579ed in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID ae91cf83ed2fa079c51be9851a9579ed in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:03:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '346', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179538', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '154ms', 'x-request-id': 'ae91cf83ed2fa079c51be9851a9579ed', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0e9bfc4b4084-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:03:39] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 3460ea1eff08b46af5d598ef31310fac in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 3460ea1eff08b46af5d598ef31310fac in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 3460ea1eff08b46af5d598ef31310fac in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:03:37 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '318', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179538', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '154ms', 'x-request-id': '3460ea1eff08b46af5d598ef31310fac', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0eddca6e4084-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:03:49] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b02f43616dbc1c9672f823a309d3a239 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b02f43616dbc1c9672f823a309d3a239 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b02f43616dbc1c9672f823a309d3a239 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:03:47 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '329', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179538', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '154ms', 'x-request-id': 'b02f43616dbc1c9672f823a309d3a239', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0f1f7ec24084-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:04:00] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID c3cc5a7e893c548ec2743cf9b2e48bfa in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID c3cc5a7e893c548ec2743cf9b2e48bfa in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID c3cc5a7e893c548ec2743cf9b2e48bfa in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:03:58 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '121', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179538', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '154ms', 'x-request-id': 'c3cc5a7e893c548ec2743cf9b2e48bfa', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0f616e3a4084-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 11-04 13:04:10] {238} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 0a8f0402029b321eaaf51199af117900 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 0a8f0402029b321eaaf51199af117900 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 0a8f0402029b321eaaf51199af117900 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sat, 04 Nov 2023 13:04:08 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-16k-0613', 'openai-organization': 'user-o32o8oehynmyzpauk2oqjtyt', 'openai-processing-ms': '244', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '180000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '179538', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '154ms', 'x-request-id': '0a8f0402029b321eaaf51199af117900', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '820d0fa2695d4084-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\Data-Centric Solutions\\07. Blog Posts\\AutoGen\\autogen_tutorial\\autogen_tutorial_v2.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/AutoGen/autogen_tutorial/autogen_tutorial_v2.ipynb#X12sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m groupchat \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mGroupChat(agents\u001b[39m=\u001b[39m[user_proxy, wikisearch_agent, wiki_index_agent, retrieve_agent, response_agent], messages\u001b[39m=\u001b[39m[], max_round\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/AutoGen/autogen_tutorial/autogen_tutorial_v2.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m manager \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mGroupChatManager(groupchat\u001b[39m=\u001b[39mgroupchat, llm_config\u001b[39m=\u001b[39mllm_config)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/AutoGen/autogen_tutorial/autogen_tutorial_v2.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(manager, message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWhich bank was the first to default in the 2023 US banking crisis?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \n\u001b[0;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[1;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[0;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[1;32m--> 781\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[0;32m    783\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\agentchat\\groupchat.py:162\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[39m# select the next speaker\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m     speaker \u001b[39m=\u001b[39m groupchat\u001b[39m.\u001b[39;49mselect_speaker(speaker, \u001b[39mself\u001b[39;49m)\n\u001b[0;32m    163\u001b[0m     \u001b[39m# let the speaker speak\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     reply \u001b[39m=\u001b[39m speaker\u001b[39m.\u001b[39mgenerate_reply(sender\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\agentchat\\groupchat.py:91\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[1;34m(self, last_speaker, selector)\u001b[0m\n\u001b[0;32m     87\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     88\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGroupChat is underpopulated with \u001b[39m\u001b[39m{\u001b[39;00mn_agents\u001b[39m}\u001b[39;00m\u001b[39m agents. Direct communication would be more efficient.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         )\n\u001b[0;32m     90\u001b[0m selector\u001b[39m.\u001b[39mupdate_system_message(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_speaker_msg(agents))\n\u001b[1;32m---> 91\u001b[0m final, name \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mgenerate_oai_reply(\n\u001b[0;32m     92\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessages\n\u001b[0;32m     93\u001b[0m     \u001b[39m+\u001b[39;49m [\n\u001b[0;32m     94\u001b[0m         {\n\u001b[0;32m     95\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     96\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRead the above conversation. Then select the next role from \u001b[39;49m\u001b[39m{\u001b[39;49;00m[agent\u001b[39m.\u001b[39;49mname\u001b[39m \u001b[39;49m\u001b[39mfor\u001b[39;49;00m\u001b[39m \u001b[39;49magent\u001b[39m \u001b[39;49m\u001b[39min\u001b[39;49;00m\u001b[39m \u001b[39;49magents]\u001b[39m}\u001b[39;49;00m\u001b[39m to play. Only return the role.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     97\u001b[0m         }\n\u001b[0;32m     98\u001b[0m     ]\n\u001b[0;32m     99\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m final:\n\u001b[0;32m    101\u001b[0m     \u001b[39m# i = self._random.randint(0, len(self._agent_names) - 1)  # randomly pick an id\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_agent(last_speaker, agents)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m    607\u001b[0m     context\u001b[39m=\u001b[39mmessages[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m), messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_system_message \u001b[39m+\u001b[39m messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mllm_config\n\u001b[0;32m    608\u001b[0m )\n\u001b[0;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[0;32m    801\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 803\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m    804\u001b[0m         context,\n\u001b[0;32m    805\u001b[0m         use_cache,\n\u001b[0;32m    806\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39mi \u001b[39m<\u001b[39m last \u001b[39mor\u001b[39;00m raise_on_ratelimit_or_timeout,\n\u001b[0;32m    807\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbase_config,\n\u001b[0;32m    808\u001b[0m     )\n\u001b[0;32m    809\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m    810\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py:834\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[0;32m    833\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[1;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\autogen\\oai\\completion.py:222\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[1;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[1;32m--> 222\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n\u001b[0;32m    223\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(request_timeout\u001b[39m=\u001b[39mrequest_timeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[0;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py:289\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[0;32m    292\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    293\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    294\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    295\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    296\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[0;32m    299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\openai\\api_requestor.py:606\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    604\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    605\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 606\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    607\u001b[0m         method,\n\u001b[0;32m    608\u001b[0m         abs_url,\n\u001b[0;32m    609\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    610\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    611\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    612\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    613\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    614\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[0;32m    615\u001b[0m     )\n\u001b[0;32m    616\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    617\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    712\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    714\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    716\u001b[0m     conn,\n\u001b[0;32m    717\u001b[0m     method,\n\u001b[0;32m    718\u001b[0m     url,\n\u001b[0;32m    719\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    720\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    721\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    722\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    723\u001b[0m )\n\u001b[0;32m    725\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    729\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    462\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    464\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    466\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    468\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\site-packages\\urllib3\\connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 462\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    463\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    464\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1272\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1273\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1274\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\johna\\anaconda3\\envs\\autogen_env\\lib\\ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1134\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import autogen \n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    system_message= '''Your job is to use agents to respond to queries.\n",
    "    You will give a final response once you are satisfied with the answer''',\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config={\"work_dir\": \"coding\"}\n",
    "    \n",
    ")\n",
    "\n",
    "# task_agent = autogen.AssistantAgent(\n",
    "#     name=\"task_agent\",\n",
    "#     system_message='''Your job is to break the task down into subtasks.\n",
    "#     Reply `TERMINATE` in the end when everything is done.''',\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "\n",
    "wikisearch_agent = autogen.AssistantAgent(\n",
    "    name=\"wikisearch_agent\",\n",
    "    system_message='''Your job is to return a search request relevant to solving the problem.\n",
    "    Reply `TERMINATE` in the end when everything is done.''',\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "wiki_index_agent = autogen.AssistantAgent(\n",
    "    name=\"wiki_index_agent\",\n",
    "    system_message='''Your job is to index the relevant wikipedia pages from the titles provided by wikisearch_agent.\n",
    "    Reply `TERMINATE` in the end when everything is done.''',\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "retrieve_agent = autogen.AssistantAgent(\n",
    "    name=\"retrieve_agent\",\n",
    "    system_message='''Your job is to retrieve relevant information from the index.\n",
    "    Reply `TERMINATE` in the end when everything is done.''',\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "response_agent = autogen.AssistantAgent(\n",
    "    name=\"response_agent\",\n",
    "    system_message='''Your job is to generate a response to the task based using the generate_response tool.\n",
    "    Reply `TERMINATE` in the end when everything is done.''',\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "user_proxy.register_function(\n",
    "    function_map={\n",
    "        \"search_wikipedia\": search_wikipedia,\n",
    "        \"index_wikipedia_pages\":index_wikipedia_pages,\n",
    "        \"query_vector_db\":query_vector_db,\n",
    "        \"generate_response\":generate_response\n",
    "    }\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, wikisearch_agent, wiki_index_agent, retrieve_agent, response_agent], messages=[], max_round=20)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "user_proxy.initiate_chat(manager, message=\"Which bank was the first to default in the 2023 US banking crisis?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
