{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Gen Tutorial\n",
    "Note book written by John Adeojo\n",
    "Founder, and Chief Data Scientist at [Data-centric Solutions](https://www.data-centric-solutions.com/).\n",
    "\n",
    "---\n",
    "# License\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "## How to Credit\n",
    "\n",
    "If you use this work or adapt it, please credit the author and the company as follows:\n",
    "\n",
    "\"Auto Gen Tutorial: Open Domain Question Answering with Wikipedia\" by John Adeojo from Data-Centric Solutions, used under CC BY 4.0 / Desaturated from original\n",
    "\n",
    "## Example Citation\n",
    "\n",
    "In academic or professional contexts, you can cite this work as follows:\n",
    "\n",
    "Adeojo, John. \"Auto Gen Tutorial: Open Domain Question Answering with Wikipedia.\" Data-Centric Solutions. 08/11/202\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "import openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change the directories to pick up the files. Ensure you use your own OpenAI API Keys\n",
    "# index_path = r\"CC:\\Users\\LENOVO\\1. Projects\\AutoGen\\autogen_tutorial\\indexes\"\n",
    "# configurations_path = r\"C:\\Users\\LENOVO\\1. Projects\\AutoGen\\autogen_tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the directories to pick up the files. Ensure you use your own OpenAI API Keys\n",
    "index_path = r\"..\\autogen_tutorial\\indexes\"\n",
    "configurations_path = r\"..\\autogen_tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"configurations.json\",\n",
    "    file_location=configurations_path,\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-1106-preview\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = config_list[0]['api_key']\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.readers.base import BaseReader\n",
    "from llama_index.readers.schema.base import Document\n",
    "import wikipedia\n",
    "\n",
    "class WikipediaReader(BaseReader):\n",
    "    def load_data(self, pages: List[str], lang: str = \"en\", **load_kwargs: Any) -> List[Document]:\n",
    "        results = []\n",
    "        for page in pages:\n",
    "            wikipedia.set_lang(lang)\n",
    "            wiki_page = wikipedia.page(page, **load_kwargs)\n",
    "            page_content = wiki_page.content\n",
    "            page_url = wiki_page.url\n",
    "            # Create a Document with URL included in the metadata\n",
    "            document = Document(text=page_content, metadata={'source_url': page_url})\n",
    "            results.append(document)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 1: Does a query based search for Wikipages\n",
    "import wikipedia\n",
    "from llama_index import download_loader, VectorStoreIndex, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.text_splitter import get_default_text_splitter\n",
    "from llama_index import StorageContext\n",
    "from llama_index import load_index_from_storage\n",
    "import json\n",
    "\n",
    "def load_index(filepath: str):\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "    # load index\n",
    "    return load_index_from_storage(storage_context)\n",
    "\n",
    "def read_json_file(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_wikidocs(wikipage_requests):\n",
    "    print(f\"Preparing to Download:{wikipage_requests}\")\n",
    "    documents = []\n",
    "    for page_title in wikipage_requests:\n",
    "        try:\n",
    "            # Attempt to load the Wikipedia page\n",
    "            wiki_page = wikipedia.page(page_title)\n",
    "            page_content = wiki_page.content\n",
    "            page_url = wiki_page.url\n",
    "            document = Document(text=page_content, metadata={'source_url': page_url})\n",
    "            documents.append(document)\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            # Handle the case where the page does not exist\n",
    "            print(f\"PageError: The page titled '{page_title}' does not exist on Wikipedia.\")\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            # Handle the case where the page title is ambiguous\n",
    "            print(f\"DisambiguationError: The page titled '{page_title}' is ambiguous. Possible options: {e.options}\")\n",
    "    print(\"Finished downloading pages\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "def index_wikipedia_pages(wikipage_requests):\n",
    "    print(f\"Preparing to index Wikipages: {wikipage_requests}\")\n",
    "    documents = create_wikidocs(wikipage_requests)\n",
    "    text_splits = get_default_text_splitter(chunk_size=150, chunk_overlap=45)\n",
    "    parser = SimpleNodeParser.from_defaults(text_splitter=text_splits)\n",
    "    service_context = ServiceContext.from_defaults(node_parser=parser)\n",
    "    index =  VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=False)\n",
    "    index.storage_context.persist(index_path)\n",
    "    print(f\"{wikipage_requests} have been indexed.\")\n",
    "    return \"indexed\"\n",
    "\n",
    "def search_and_index_wikipedia(\n",
    "        hops: list, lang: str = \"en\", results_limit: int = 2\n",
    "    ):\n",
    "\n",
    "    # Set the language for Wikipedia\n",
    "    wikipedia.set_lang(lang)\n",
    "\n",
    "    # Initialize an empty list to hold all indexed page titles\n",
    "    wikipage_requests = []\n",
    "\n",
    "    # Loop through the identified hops and search for each\n",
    "    for hop in hops:\n",
    "        hop_pages = wikipedia.search(hop, results=results_limit)\n",
    "        print(f\"Searching Wikipedia for: {hop} - Found: {hop_pages}\")\n",
    "        wikipage_requests.extend(hop_pages)\n",
    "\n",
    "    # Index the gathered pages (assuming 'index_wikipedia_pages' is a defined function that you implement)\n",
    "    index_wikipedia_pages(wikipage_requests)\n",
    "\n",
    "    return wikipage_requests\n",
    "\n",
    "\n",
    "def query_wiki_index(hops: List[str], index_path: str = index_path, n_results: int = 5): \n",
    "    index = load_index(filepath=index_path)\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"compact\", verbose=True, similarity_top_k=n_results\n",
    "    )\n",
    "    \n",
    "    retrieved_context = {}\n",
    "    \n",
    "    # Iterate over each hop in the multihop query\n",
    "    for hop in hops:\n",
    "        nodes = query_engine.query(hop).source_nodes\n",
    "        \n",
    "        # Process each node found for the current hop\n",
    "        for node in nodes:\n",
    "            doc_id = node.node.id_\n",
    "            doc_text = node.node.text\n",
    "            doc_source = node.node.metadata.get('source_url', 'No source URL')  # Default value if source_url is not present.\n",
    "            \n",
    "            # Append to the list of texts and sources for each doc_id\n",
    "            if doc_id not in retrieved_context:\n",
    "                retrieved_context[doc_id] = {'texts': [doc_text], 'sources': [doc_source]}\n",
    "            else:\n",
    "                retrieved_context[doc_id]['texts'].append(doc_text)\n",
    "                retrieved_context[doc_id]['sources'].append(doc_source)\n",
    "\n",
    "    # Serialise the context for all hops into a JSON file\n",
    "    file_path = index_path + \"retrieved_context.json\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(retrieved_context, f)\n",
    "    \n",
    "    return retrieved_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_config = {\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"search_and_index_wikipedia\",\n",
    "            \"description\": \"Indexes Wikipedia pages based on specified queries for each hop to build a knowledge base for future reference. Use before query_wiki_index.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"hops\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"description\": \"The search queries for identifying relevant Wikipedia pages to index, each corresponding to a hop in the multihop question.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"hops\"],\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"query_wiki_index\",\n",
    "            \"description\": \"Queries the indexed Wikipedia knowledge base to retrieve pertinent information across multiple hops\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"hops\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"description\": \"The search queries to search the indexed Wikipedia knowledge base for relevant information, each corresponding to a hop in the multihop question.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"hops\"],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "    \"request_timeout\": 120,\n",
    "    \"seed\": 100,\n",
    "    \"temperature\":0.7\n",
    "}\n",
    "\n",
    "# The llm_config_no_tools remains the same, excluding the 'functions' key.\n",
    "llm_config_no_tools = {k: v for k, v in llm_config.items() if k != 'functions'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Agent Worflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen \n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    # system_message='''You should start the workflow by consulting the analyst, then the reporter and finally the moderator. \n",
    "    # If the analyst does not use both the `search_and_index_wikipedia` and the `query_wiki_index`, you must request that it does.'''\n",
    "    \n",
    ")\n",
    "\n",
    "analyst = autogen.AssistantAgent(\n",
    "    name=\"analyst\",\n",
    "    system_message='''\n",
    "    As the Information Gatherer, you must start by using the `search_and_index_wikipedia` function to gather relevant data about the user's query. Follow these steps:\n",
    "\n",
    "    1. Upon receiving a query, immediately invoke the `search_and_index_wikipedia` function to find and index Wikipedia pages related to the query. Do not proceed without completing this step.\n",
    "    2. After successfully indexing, utilize the `query_wiki_index` to extract detailed information from the indexed content.\n",
    "    3. Present the indexed information and detailed findings to the Reporter, ensuring they have a comprehensive dataset to draft a response.\n",
    "    4. Conclude your part with \"INFORMATION GATHERING COMPLETE\" to signal that you have finished collecting data and it is now ready for the Reporter to use in formulating the answer.\n",
    "\n",
    "    Remember, you are responsible for information collection and indexing only. The Reporter will rely on the accuracy and completeness of your findings to generate the final answer.\n",
    "\n",
    "    ''',\n",
    "    llm_config=llm_config,\n",
    "    # human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "reporter = autogen.AssistantAgent(\n",
    "    name=\"reporter\",\n",
    "    system_message='''\n",
    "    As the Reporter, you are responsible for formulating an answer to the user's query using the information provided by the Information Gatherer.\n",
    "\n",
    "    1. Wait for the Information Gatherer to complete their task and present you with the indexed information.\n",
    "    2. Using the gathered data, create a comprehensive and precise response that adheres to the criteria of precision, depth, clarity, and proper citation.\n",
    "    3. Present your draft answer followed by \"PLEASE REVIEW\" for the Moderator to assess.\n",
    "\n",
    "    If the Moderator approves your answer, respond with \"TERMINATE\" to signal the end of the interaction.\n",
    "\n",
    "    If the Moderator rejects your answer:\n",
    "    - Review their feedback.\n",
    "    - Make necessary amendments.\n",
    "    - Resubmit the revised answer with \"PLEASE REVIEW.\"\n",
    "\n",
    "    Ensure that your response is fully informed by the data provided and meets the established criteria.\n",
    "\n",
    "    criteria are as follows:\n",
    "     A. Precision: Directly address the user's question.\n",
    "     B. Depth: Provide comprehensive information using indexed content.\n",
    "     C. Citing: Incorporate citations within your response using the Vancouver citation style. \n",
    "     For each reference, a superscript number shoud be insered in the text at the point of citation, corresponding to the number of the reference. \n",
    "     At the end of the document, references must be listed numerically with links to the source provided. \n",
    "     For instance, if you are citing a Wikipedia article, it would look like this in the text:\n",
    "\n",
    "        \"The collapse of Silicon Valley Bank was primarily due to...[1].\"\n",
    "\n",
    "        And then at the end of the document:\n",
    "\n",
    "        References\n",
    "        1. Wikipedia Available from: https://en.wikipedia.org/wiki/Collapse_of_Silicon_Valley_Bank.\n",
    "\n",
    "        Ensure that each citation number corresponds to a unique reference which is listed at the end of your report in the order they appear in the text.\n",
    "          D. Clarity: Present information logically and coherently.\n",
    "\n",
    "    ''',\n",
    "    llm_config=llm_config_no_tools,\n",
    "    \n",
    ")\n",
    "\n",
    "moderator = autogen.AssistantAgent(\n",
    "    name=\"moderator\",\n",
    "    system_message='''\n",
    "\n",
    "    As the Moderator, your task is to review the Reporter's answers to ensure they meet the required criteria:\n",
    "\n",
    "    - Assess the Reporter's answers after the \"PLEASE REVIEW\" prompt for alignment with the following criteria:\n",
    "     A. Precision: Directly addressed the user's question.\n",
    "     B. Depth: Provided comprehensive information using indexed content.\n",
    "     C. Citing: Citations should be encorporated using the Vancouver citation style. \n",
    "     For each reference, a superscript number shoud be insered in the text at the point of citation, corresponding to the number of the reference. \n",
    "     At the end of the document, references must be listed numerically with links to the source provided. \n",
    "     For instance, if you are citing a Wikipedia article, it would look like this in the text:\n",
    "\n",
    "        \"The collapse of Silicon Valley Bank was primarily due to...[1].\"\n",
    "\n",
    "        And then at the end of the document:\n",
    "\n",
    "        References\n",
    "        1. Wikipedia Available from: https://en.wikipedia.org/wiki/Collapse_of_Silicon_Valley_Bank.\n",
    "\n",
    "        Ensure that each citation number corresponds to a unique reference which is listed at the end of your report in the order they appear in the text.\n",
    "     \n",
    "     D. Clarity: information presented logically and coherently.\n",
    "    - Approve the answer by stating \"The answer is approved\" if it meets the criteria.\n",
    "    - If the answer falls short, specify which criteria were not met and instruct the Reporter to revise the answer accordingly. Do not generate new content or answers yourself.\n",
    "\n",
    "    Your role is crucial in ensuring that the final answer provided to the user is factually correct and meets all specified quality standards.\n",
    "\n",
    "    ''',\n",
    "    llm_config=llm_config_no_tools,\n",
    ")\n",
    "\n",
    "user_proxy.register_function(\n",
    "    function_map={\n",
    "        \"search_and_index_wikipedia\": search_and_index_wikipedia,\n",
    "        \"query_wiki_index\":query_wiki_index,\n",
    "    }\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, analyst, reporter, moderator], \n",
    "    messages=[], \n",
    "    max_round=20\n",
    "    )\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat, \n",
    "    llm_config=llm_config, \n",
    "    system_message='''You should start the workflow by consulting the analyst, \n",
    "    then the reporter and finally the moderator. \n",
    "    If the analyst does not use both the `search_and_index_wikipedia` \n",
    "    and the `query_wiki_index`, you must request that it does.'''\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mchat_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "What is AutoGen in AI\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33manalyst\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: search_and_index_wikipedia *****\u001b[0m\n",
      "Arguments: \n",
      "{\"hops\":[\"AutoGen in AI\"]}\n",
      "\u001b[32m***************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_and_index_wikipedia...\u001b[0m\n",
      "Searching Wikipedia for: AutoGen in AI - Found: ['Stable Diffusion', 'Grand Theft Auto V']\n",
      "Preparing to index Wikipages: ['Stable Diffusion', 'Grand Theft Auto V']\n",
      "Preparing to Download:['Stable Diffusion', 'Grand Theft Auto V']\n",
      "Finished downloading pages\n",
      "['Stable Diffusion', 'Grand Theft Auto V'] have been indexed.\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"search_and_index_wikipedia\" *****\u001b[0m\n",
      "['Stable Diffusion', 'Grand Theft Auto V']\n",
      "\u001b[32m***********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33manalyst\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: query_wiki_index *****\u001b[0m\n",
      "Arguments: \n",
      "{\"hops\":[\"AutoGen in AI\"]}\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION query_wiki_index...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"query_wiki_index\" *****\u001b[0m\n",
      "{'f34a052e-6b40-46a6-a54f-839920113804': {'texts': ['However, individuals depicted in generated images may be protected by personality rights if their likeness is used, and intellectual property such as recognizable brand logos still remain protected by copyright. Nonetheless, visual artists have expressed concern that widespread usage of image synthesis software such as Stable Diffusion may eventually lead to human artists, along with photographers, models, cinematographers, and actors, gradually losing commercial viability against AI-based competitors.Stable Diffusion is notably more permissive in the types of content users may generate, such as violent or sexually explicit imagery, in comparison to other commercial products based on generative AI.'], 'sources': ['https://en.wikipedia.org/wiki/Stable_Diffusion']}, '27d9fdc4-2994-4758-a5f5-9fb0d0e27228': {'texts': ['The graphical and artistic design received awards from IGN, The Daily Telegraph and BAFTA, and a nomination at the Game Developers Choice Awards. The Academy of Interactive Arts & Sciences awarded the game with Outstanding Achievement in Gameplay Engineering, signifying \"the highest level of achievement for engineering artificial intelligence and related elements which contribute to a challenging game\".The game received numerous other awards. It was awarded the title of Most Immersive Game at the Inside Gaming Awards. The general public voted for the game to win the User Choice Award at the PlayStation Awards 2013 and the Community Choice award from Destructoid.'], 'sources': ['https://en.wikipedia.org/wiki/Grand_Theft_Auto_V']}, '3ce6831d-3215-43af-a04c-7c181813fca2': {'texts': ['Though players complete missions with the protagonists, the more difficult heist missions require aid from AI-controlled accomplices with unique skill sets like computer hacking and driving. If an accomplice survives a successful heist, they take a cut from the cash reward and may be available for later missions with improvements to their unique skills. Some heists afford multiple strategies; in a holdup mission, players may either stealthily subdue civilians with an incapacitating agent or conspicuously storm the venue with guns drawn.Each character has a set of eight skills that represent their ability in specific areas such as shooting and driving.'], 'sources': ['https://en.wikipedia.org/wiki/Grand_Theft_Auto_V']}, '2b20285b-b1f1-497a-9b25-483299c7cda3': {'texts': ['It applies the Creative ML OpenRAIL-M license, a form of Responsible AI License (RAIL), to the model (M). The license prohibits certain use cases, including crime, libel, harassment, doxing, \"exploiting ... minors\", giving medical advice, automatically creating legal obligations, producing legal evidence, and \"discriminating against or harming individuals or groups based on ... social behavior or ... personal or personality characteristics ... [or] legally protected characteristics or categories\". The user owns the rights to their generated output images, and is free to use them commercially.'], 'sources': ['https://en.wikipedia.org/wiki/Stable_Diffusion']}, 'cacc4102-e0ff-4143-8be6-8864c9a9a1b3': {'texts': ['Development was led by Patrick Esser of Runway and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion. Stability AI also credited EleutherAI and LAION (a German nonprofit which assembled the dataset on which Stable Diffusion was trained) as supporters of the project.In October 2022, Stability AI raised US$101 million in a round led by Lightspeed Venture Partners and Coatue Management.\\n\\n\\n== Technology =='], 'sources': ['https://en.wikipedia.org/wiki/Stable_Diffusion']}}\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mreporter\u001b[0m (to chat_manager):\n",
      "\n",
      "The term \"AutoGen in AI\" does not appear to directly correspond to a specific concept, technology, or product based on the indexed information provided. However, the concept of \"AutoGen\" could potentially refer to the automatic generation of content using artificial intelligence, which is a broad area within AI research and application. While the indexed information does not include a direct mention of \"AutoGen,\" it does touch upon related technologies like Stable Diffusion, which is an image synthesis software that uses AI to generate images[1]. Stable Diffusion utilizes generative AI to enable users to create a wide spectrum of visual content, including those that are violent or sexually explicit, offering more permissive content generation compared to other commercial products[1].\n",
      "\n",
      "To provide a more comprehensive response, \"AutoGen\" in the context of AI could theoretically refer to any system that automatically generates content, code, data, or other outputs using AI techniques. This might include generative models like those used in Stable Diffusion for images, text generation models for writing, or even systems in gaming that create dynamic experiences, such as the AI-controlled accomplices in Grand Theft Auto V that have unique skill sets and can participate in complex missions[2][3].\n",
      "\n",
      "If you are referring to a specific technology, product, or concept named \"AutoGen\" within the AI industry, additional context or details would be necessary to provide an accurate and targeted answer.\n",
      "\n",
      "PLEASE REVIEW\n",
      "\n",
      "References:\n",
      "1. Wikipedia. Available from: https://en.wikipedia.org/wiki/Stable_Diffusion\n",
      "2. Wikipedia. Available from: https://en.wikipedia.org/wiki/Grand_Theft_Auto_V\n",
      "3. Wikipedia. Available from: https://en.wikipedia.org/wiki/Stable_Diffusion\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mmoderator\u001b[0m (to chat_manager):\n",
      "\n",
      "The answer is approved.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mreporter\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "manager.initiate_chat(\n",
    "    manager, \n",
    "    message='''What is AutoGen in AI'''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
